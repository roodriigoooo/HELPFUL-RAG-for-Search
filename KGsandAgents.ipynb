{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7be763ac-f2be-4f60-b412-80d2d48335ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from networkx import DiGraph\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "import glob\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from preprocessing import SemanticJSONSplitter, load_and_process_documents\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import numpy as np\n",
    "from crewai import Agent, Task, Crew, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31b161a7-d4d3-46cb-9fe2-5919c9283786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryType(Enum):\n",
    "    SEMANTIC = \"semantic\"\n",
    "    KEYWORD = \"keyword\"\n",
    "    GRAPH = \"graph\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class MakerspaceKG:\n",
    "    def __init__(self, embeddings: HuggingFaceEmbeddings):\n",
    "        self.graph = DiGraph()\n",
    "        self.node_embeddings = {}\n",
    "        self.embeddings = embeddings  # i use the same original embeddings model\n",
    "    \n",
    "    def get_subgraph_for_query(self, query: str) -> DiGraph:\n",
    "        # get query embedding using our embedding model instance\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Find relevant nodes\n",
    "        relevant_nodes = []\n",
    "        for node in self.graph.nodes:\n",
    "            if node in self.node_embeddings:\n",
    "                similarity = self._calculate_similarity(\n",
    "                    query_embedding, \n",
    "                    self.node_embeddings[node]\n",
    "                )\n",
    "                if similarity > 0.5:  # Threshold can be adjusted\n",
    "                    relevant_nodes.append(node)\n",
    "        \n",
    "        return nx.subgraph(self.graph, relevant_nodes)\n",
    "        \n",
    "    def _calculate_similarity(self, embedding1, embedding2):\n",
    "        \"\"\"Calculate cosine similarity between two embeddings\"\"\"\n",
    "        return np.dot(embedding1, embedding2) / (\n",
    "            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n",
    "        )\n",
    "\n",
    "    def add_node_with_embedding(self, name: str, description: str, **attrs):\n",
    "        \"\"\"Add a node and compute its embedding\"\"\"\n",
    "        self.graph.add_node(name, **attrs)\n",
    "        self.node_embeddings[name] = self.embeddings.embed_query(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb1c85a4-dd37-4a58-9444-f6a059cbe475",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "class AutoRAGRouter:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    def determine_query_type(self, query: str) -> QueryType:\n",
    "        # Analyze query complexity and structure\n",
    "        query_features = self._extract_query_features(query)\n",
    "        \n",
    "        if query_features['requires_graph_traversal']:\n",
    "            return QueryType.GRAPH\n",
    "        elif query_features['is_semantic']:\n",
    "            return QueryType.SEMANTIC\n",
    "        elif query_features['is_keyword_based']:\n",
    "            return QueryType.KEYWORD\n",
    "        else:\n",
    "            return QueryType.HYBRID\n",
    "    \n",
    "    def _extract_query_features(self, query: str) -> Dict[str, bool]:\n",
    "        # Implement query analysis logic\n",
    "        # This is a simplified version\n",
    "        words = query.lower().split()\n",
    "        \n",
    "        return {\n",
    "            'requires_graph_traversal': any(w in words for w in ['related', 'connected', 'similar']),\n",
    "            'is_semantic': len(words) > 5 and ' '.join(words).find(' with ') != -1,\n",
    "            'is_keyword_based': len(words) <= 3\n",
    "        }\n",
    "'''\n",
    "class QueryAnalysisAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role='Query Analyst',\n",
    "            goal='Analyze and refine search queries to improve retrieval effectiveness',\n",
    "            backstory=\"\"\"You are an expert at understanding user queries and breaking them down \n",
    "            into structured components that can be effectively searched across different information sources.\"\"\",\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            tools=[\n",
    "                Tool(\n",
    "                    name='analyze_query',\n",
    "                    func=self.analyze_query,\n",
    "                    description='Analyze and structure the search query'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def analyze_query(self, query:str) -> Dict[str,Any]:\n",
    "        '''analyze the query and break it dowm into structured, usable components'''\n",
    "        analysis_prompt=f\"\"\"\n",
    "        Analyze the following query and break it down into components:\n",
    "        Query: {query}\n",
    "\n",
    "        Provide:\n",
    "        1. Main topics/concepts\n",
    "        2. Required capabilities. \n",
    "        3. Required resources. \n",
    "        4. Any constraints or preferences. \n",
    "        5. Query type (semantic, keyword-based, or relationship-focused)\n",
    "\n",
    "        Format as JSON. \n",
    "        \"\"\"\n",
    "        result = self.execute_task(analysis_prompt)\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except:\n",
    "            return {\n",
    "                \"main_topics\": [],\n",
    "                \"capabilities\": [],\n",
    "                \"resources\": [],\n",
    "                \"constraints\": {},\n",
    "                \"query_type\": \"semantic\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edde83af-3a5f-453c-9d0b-14bfb32d4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphAgent(Agent):\n",
    "    def __init__(self, llm, knowledge_graph: MakerspaceKG):\n",
    "        super().__init__(\n",
    "            role='Knowledge Graph Explorer',\n",
    "            goal='Navigate and extract relevant information from the knowledge graph',\n",
    "            backstory=\"\"\"You are an expert at traversing knowledge graphs and finding\n",
    "            connections between different concepts and entities.\"\"\",\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            tools=[\n",
    "                Tool(\n",
    "                    name=\"explore_graph\",\n",
    "                    func=self.explore_graph,\n",
    "                    description=\"Explore the knowledge graph based on query analysis\"\n",
    "                )\n",
    "            ],\n",
    "            memory={'knowledge_graph': knowledge_graph}\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def explore_graph(self, query_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Explore the knowledge graph based on query analysis\"\"\"\n",
    "        kg = self.memory.get('knowledge_graph')\n",
    "        \n",
    "        # Create a comprehensive query from the analysis\n",
    "        kg_query = ' '.join([\n",
    "            *query_analysis.get('main_topics', []),\n",
    "            *query_analysis.get('capabilities', []),\n",
    "            *query_analysis.get('resources', [])\n",
    "        ])\n",
    "        \n",
    "        subgraph = self.kg.get_subgraph_for_query(kg_query)\n",
    "        results = []\n",
    "        for node in subgraph.nodes:\n",
    "            node_data = subgraph.nodes[node]\n",
    "            results.append({\n",
    "                'name': node,\n",
    "                'type': 'graph_node',\n",
    "                'data': node_data\n",
    "            })\n",
    "        return results\n",
    "\n",
    "class DocumentRetrievalAgent(Agent):\n",
    "    def __init__(self, llm, vector_store: Chroma):\n",
    "        super().__init__(\n",
    "            role='Document Retriever',\n",
    "            goal='Retrieve and rank relevant documents based on query analysis',\n",
    "            backstory=\"\"\"You are an expert at finding the most relevant documents\n",
    "            and ranking them based on their relevance to the query.\"\"\",\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            tools=[\n",
    "                Tool(\n",
    "                    name='retrieve_documents',\n",
    "                    func=self.retrieve_documents, \n",
    "                    description='Retrieve relevant documents based on query analysis'\n",
    "                )\n",
    "            ],\n",
    "            memory = {'vector_store': vector_store}\n",
    "        )\n",
    "\n",
    "        \n",
    "    def retrieve_documents(self, query_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents based on query analysis\"\"\"\n",
    "        # Combine different aspects of the query for comprehensive search\n",
    "        vector_store = self.memory.get('vector_store')\n",
    "        \n",
    "        search_query = ' '.join([\n",
    "            *query_analysis.get('main_topics', []),\n",
    "            *query_analysis.get('capabilities', []),\n",
    "            *query_analysis.get('resources', [])\n",
    "        ])\n",
    "        \n",
    "        results = vector_store.similarity_search(search_query, k=5)\n",
    "        return [\n",
    "            {\n",
    "                'title': doc.metadata.get('title', 'No Title'),\n",
    "                'content': doc.page_content,\n",
    "                'type': 'document',\n",
    "                'metadata': doc.metadata\n",
    "            }\n",
    "            for doc in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb7d765a-6dec-4bd2-8ed3-77c107776cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SearchAgent:\\n    def __init__(self, llm, knowledge_graph: MakerspaceKG, vector_store):\\n        self.llm = llm\\n        self.kg = knowledge_graph\\n        self.vector_store = vector_store\\n        self.router = AutoRAGRouter()\\n        \\n    def search(self, query: str) -> List[Dict[str, Any]]:\\n        # Determine search strategy\\n        query_type = self.router.determine_query_type(query)\\n        \\n        # Execute appropriate search strategy\\n        if query_type == QueryType.GRAPH:\\n            results = self._graph_search(query)\\n        elif query_type == QueryType.SEMANTIC:\\n            results = self._semantic_search(query)\\n        elif query_type == QueryType.KEYWORD:\\n            results = self._keyword_search(query)\\n        else:\\n            results = self._hybrid_search(query)\\n\\n        if results is None:\\n            results = []\\n\\n        return self._format_results(results)\\n    \\n    def _graph_search(self, query: str) -> List[Any]:\\n        try:\\n            subgraph = self.kg.get_subgraph_for_query(query, self.model)\\n            if not subgraph:\\n                return []\\n\\n            results = []\\n            for node in subgraph.nodes:\\n                node_data = subgraph.nodes[node]\\n                results.append({\\n                    \\'name\\': node,\\n                    \\'type\\': node_data.get(\\'type\\', \\'unknown\\'),\\n                    \\'capabilities\\': node_data.get(\\'capabilities\\', []),\\n                    \\'resources\\': node_data.get(\\'resources\\', [])\\n                })\\n            return results\\n        except Exception as e:\\n            print(f\"Graph search error: {e}\")\\n            return []\\n    \\n    def _semantic_search(self, query: str) -> List[Document]:\\n        try:\\n            results = self.vector_store.similarity_search(query)\\n            return results if results else []\\n        except Exception as e:\\n            print(f\"Semantic search error: {e}\")\\n            return []\\n    \\n    def _keyword_search(self, query: str) -> List[Document]:\\n        # Implement basic keyword search using vector store\\n        try:\\n            # Split query into keywords\\n            keywords = query.lower().split()\\n            results = self.vector_store.similarity_search(\\n                \\' OR \\'.join(keywords),\\n                k=5\\n            )\\n            return results if results else []\\n        except Exception as e:\\n            print(f\"Keyword search error: {e}\")\\n            return []\\n    \\n    def _hybrid_search(self, query: str) -> List[Any]:\\n        # Combine multiple search strategies\\n        try:\\n            graph_results = self._graph_search(query)\\n            semantic_results = self._semantic_search(query)\\n            return self._merge_results(graph_results, semantic_results)\\n        except Exception as e:\\n            print(f\"Hybrid search error: {e}\")\\n            return []\\n\\n    def _format_results(self, results: List[Any]) -> List[Dict[str, Any]]:\\n        formatted_results = []\\n        \\n        for result in results:\\n            if isinstance(result, Document):\\n                # Handle document objects (from langchain documents object type)\\n                formatted_results.append({\\n                    \\'title\\': result.metadata.get(\\'title\\', \\'No Title\\'),\\n                    \\'content\\': result.page_content,\\n                    \\'type\\': \\'document\\',\\n                    \\'metadata\\': result.metadata\\n                })\\n            elif isinstance(result, dict):\\n                # Handle graph search results\\n                formatted_results.append({\\n                    \\'title\\': result.get(\\'name\\', \\'No Name\\'),\\n                    \\'capabilities\\': result.get(\\'capabilities\\', []),\\n                    \\'resources\\': result.get(\\'resources\\', []),\\n                    \\'type\\': \\'graph_node\\',\\n                    \\'metadata\\': result\\n                })\\n                \\n        return formatted_results\\n    \\n    def _merge_results(self, graph_results: List[Any], semantic_results: List[Document]) -> List[Any]:\\n        \"\"\"Merge results from different search strategies\"\"\"\\n        all_results = []\\n        \\n        # Add graph results\\n        all_results.extend(graph_results)\\n        \\n        # Add semantic results, avoiding duplicates\\n        seen_titles = {r.get(\\'title\\') for r in all_results}\\n        for doc in semantic_results:\\n            if doc.metadata.get(\\'title\\') not in seen_titles:\\n                all_results.append(doc)\\n                seen_titles.add(doc.metadata.get(\\'title\\'))\\n        \\n        return all_results\\n\\nclass MakerspaceMatchingAgent:\\n    def __init__(self, search_agent: SearchAgent):\\n        self.search_agent = search_agent\\n        \\n    def find_optimal_makerspaces(self, requirements: Dict[str, Any]) -> List[Dict[str, Any]]:\\n        # Analyze requirements\\n        required_capabilities = requirements.get(\\'capabilities\\', [])\\n        required_resources = requirements.get(\\'resources\\', [])\\n        \\n        # Search for potential matches\\n        matches = []\\n        for capability in required_capabilities:\\n            results = self.search_agent.search(capability)\\n            matches.extend(results)\\n        \\n        # here, we would ideally score and rank matches, we can use an open-source reranking model, or develop the algorithm by scratch\\n        #ranked_matches = self._rank_matches(matches, requirements)\\n        #return ranked_matches\\n        return matches\\n    \\n    def _rank_matches(self, matches: List[Dict[str, Any]], \\n                     requirements: Dict[str, Any]) -> List[Dict[str, Any]]:\\n        # Implement ranking logic\\n        pass\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class SearchAgent:\n",
    "    def __init__(self, llm, knowledge_graph: MakerspaceKG, vector_store):\n",
    "        self.llm = llm\n",
    "        self.kg = knowledge_graph\n",
    "        self.vector_store = vector_store\n",
    "        self.router = AutoRAGRouter()\n",
    "        \n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        # Determine search strategy\n",
    "        query_type = self.router.determine_query_type(query)\n",
    "        \n",
    "        # Execute appropriate search strategy\n",
    "        if query_type == QueryType.GRAPH:\n",
    "            results = self._graph_search(query)\n",
    "        elif query_type == QueryType.SEMANTIC:\n",
    "            results = self._semantic_search(query)\n",
    "        elif query_type == QueryType.KEYWORD:\n",
    "            results = self._keyword_search(query)\n",
    "        else:\n",
    "            results = self._hybrid_search(query)\n",
    "\n",
    "        if results is None:\n",
    "            results = []\n",
    "\n",
    "        return self._format_results(results)\n",
    "    \n",
    "    def _graph_search(self, query: str) -> List[Any]:\n",
    "        try:\n",
    "            subgraph = self.kg.get_subgraph_for_query(query, self.model)\n",
    "            if not subgraph:\n",
    "                return []\n",
    "\n",
    "            results = []\n",
    "            for node in subgraph.nodes:\n",
    "                node_data = subgraph.nodes[node]\n",
    "                results.append({\n",
    "                    'name': node,\n",
    "                    'type': node_data.get('type', 'unknown'),\n",
    "                    'capabilities': node_data.get('capabilities', []),\n",
    "                    'resources': node_data.get('resources', [])\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Graph search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _semantic_search(self, query: str) -> List[Document]:\n",
    "        try:\n",
    "            results = self.vector_store.similarity_search(query)\n",
    "            return results if results else []\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _keyword_search(self, query: str) -> List[Document]:\n",
    "        # Implement basic keyword search using vector store\n",
    "        try:\n",
    "            # Split query into keywords\n",
    "            keywords = query.lower().split()\n",
    "            results = self.vector_store.similarity_search(\n",
    "                ' OR '.join(keywords),\n",
    "                k=5\n",
    "            )\n",
    "            return results if results else []\n",
    "        except Exception as e:\n",
    "            print(f\"Keyword search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _hybrid_search(self, query: str) -> List[Any]:\n",
    "        # Combine multiple search strategies\n",
    "        try:\n",
    "            graph_results = self._graph_search(query)\n",
    "            semantic_results = self._semantic_search(query)\n",
    "            return self._merge_results(graph_results, semantic_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Hybrid search error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _format_results(self, results: List[Any]) -> List[Dict[str, Any]]:\n",
    "        formatted_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            if isinstance(result, Document):\n",
    "                # Handle document objects (from langchain documents object type)\n",
    "                formatted_results.append({\n",
    "                    'title': result.metadata.get('title', 'No Title'),\n",
    "                    'content': result.page_content,\n",
    "                    'type': 'document',\n",
    "                    'metadata': result.metadata\n",
    "                })\n",
    "            elif isinstance(result, dict):\n",
    "                # Handle graph search results\n",
    "                formatted_results.append({\n",
    "                    'title': result.get('name', 'No Name'),\n",
    "                    'capabilities': result.get('capabilities', []),\n",
    "                    'resources': result.get('resources', []),\n",
    "                    'type': 'graph_node',\n",
    "                    'metadata': result\n",
    "                })\n",
    "                \n",
    "        return formatted_results\n",
    "    \n",
    "    def _merge_results(self, graph_results: List[Any], semantic_results: List[Document]) -> List[Any]:\n",
    "        \"\"\"Merge results from different search strategies\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Add graph results\n",
    "        all_results.extend(graph_results)\n",
    "        \n",
    "        # Add semantic results, avoiding duplicates\n",
    "        seen_titles = {r.get('title') for r in all_results}\n",
    "        for doc in semantic_results:\n",
    "            if doc.metadata.get('title') not in seen_titles:\n",
    "                all_results.append(doc)\n",
    "                seen_titles.add(doc.metadata.get('title'))\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "class MakerspaceMatchingAgent:\n",
    "    def __init__(self, search_agent: SearchAgent):\n",
    "        self.search_agent = search_agent\n",
    "        \n",
    "    def find_optimal_makerspaces(self, requirements: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        # Analyze requirements\n",
    "        required_capabilities = requirements.get('capabilities', [])\n",
    "        required_resources = requirements.get('resources', [])\n",
    "        \n",
    "        # Search for potential matches\n",
    "        matches = []\n",
    "        for capability in required_capabilities:\n",
    "            results = self.search_agent.search(capability)\n",
    "            matches.extend(results)\n",
    "        \n",
    "        # here, we would ideally score and rank matches, we can use an open-source reranking model, or develop the algorithm by scratch\n",
    "        #ranked_matches = self._rank_matches(matches, requirements)\n",
    "        #return ranked_matches\n",
    "        return matches\n",
    "    \n",
    "    def _rank_matches(self, matches: List[Dict[str, Any]], \n",
    "                     requirements: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        # Implement ranking logic\n",
    "        pass\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b94ffa3-8f3f-40c3-ad86-577aa5cfb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseSynthesisAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role='Response Synthesizer',\n",
    "            goal='Synthesize information from multiple sources into coherent responses',\n",
    "            backstory=\"\"\"You are an expert at combining and synthesizing information\n",
    "            from multiple sources to create comprehensive and accurate responses.\"\"\",\n",
    "            llm=llm,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    def synthesize_response(self, \n",
    "                          query: str,\n",
    "                          graph_results: List[Dict[str, Any]],\n",
    "                          doc_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Synthesize final response from multiple sources\"\"\"\n",
    "        synthesis_prompt = f\"\"\"\n",
    "        Based on the following information:\n",
    "        \n",
    "        Query: {query}\n",
    "        \n",
    "        Graph Results: {json.dumps(graph_results, indent=2)}\n",
    "        \n",
    "        Document Results: {json.dumps(doc_results, indent=2)}\n",
    "        \n",
    "        Create a comprehensive response that:\n",
    "        1. Ranks the most relevant matches\n",
    "        2. Explains why each match is relevant\n",
    "        3. Highlights any special capabilities or resources\n",
    "        \n",
    "        Format as JSON with the following structure:\n",
    "        {{\n",
    "            \"matches\": [\n",
    "                {{\n",
    "                    \"title\": \"string\",\n",
    "                    \"relevance_score\": float,\n",
    "                    \"explanation\": \"string\",\n",
    "                    \"capabilities\": [\"string\"],\n",
    "                    \"resources\": [\"string\"]\n",
    "                }}\n",
    "            ],\n",
    "            \"summary\": \"string\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.execute_task(synthesis_prompt)\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except:\n",
    "            return {\n",
    "                \"matches\": [],\n",
    "                \"summary\": \"Error synthesizing response\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cc78092-e4c6-48ec-bf6a-2dde5cdf4811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Error executing crew tasks: 'Crew' object has no attribute 'execute'\n",
      "Found matches:\n",
      "\n",
      "Summary: Error occurred during processing\n"
     ]
    }
   ],
   "source": [
    "class EnhancedMakerspaceMatchingAgent:\n",
    "    def __init__(self, llm, knowledge_graph: MakerspaceKG, vector_store: Chroma):\n",
    "        # Initialize individual agents\n",
    "        self.query_analyst = QueryAnalysisAgent(llm)\n",
    "        self.kg_explorer = KnowledgeGraphAgent(llm, knowledge_graph)\n",
    "        self.doc_retriever = DocumentRetrievalAgent(llm, vector_store)\n",
    "        self.synthesizer = ResponseSynthesisAgent(llm)\n",
    "        \n",
    "        # Create the crew\n",
    "        self.crew = Crew(\n",
    "            agents=[\n",
    "                self.query_analyst,\n",
    "                self.kg_explorer,\n",
    "                self.doc_retriever,\n",
    "                self.synthesizer\n",
    "            ],\n",
    "            process=Process.sequential  # Tasks will be executed in sequence\n",
    "        )\n",
    "        \n",
    "    def find_optimal_makerspaces(self, query: str) -> Dict[str, Any]:\n",
    "        # Create tasks for the crew\n",
    "        tasks = [\n",
    "            Task(\n",
    "                description=\"Analyze and structure the search query\",\n",
    "                agent=self.query_analyst,\n",
    "                expected_output='A JSON object containing query analysis',\n",
    "                context=[{\n",
    "                    \"description\": \"Analyze user query for search\",\n",
    "                    \"expected_output\": \"Query analysis in JSON format\",\n",
    "                    \"query\": query\n",
    "                }]\n",
    "            ),\n",
    "            Task(\n",
    "                description=\"Explore knowledge graph for relevant entities\",\n",
    "                agent=self.kg_explorer,\n",
    "                expected_output='A list of relevant entities from the knowledge graph',\n",
    "                context=[{\n",
    "                    \"description\": \"Find relevant entities in knowledge graph\",\n",
    "                    \"expected_output\": \"List of matching entities\",\n",
    "                    \"query_analysis\": \"Will be filled from previous task\"\n",
    "                }]\n",
    "            ),\n",
    "            Task(\n",
    "                description=\"Retrieve relevant documents\",\n",
    "                agent=self.doc_retriever,\n",
    "                expected_output='A list of relevant documents',\n",
    "                context=[{\n",
    "                    \"description\": \"Search for relevant documents\",\n",
    "                    \"expected_output\": \"List of matching documents\",\n",
    "                    \"query_analysis\": \"Will be filled from previous task\"\n",
    "                }]\n",
    "            ),\n",
    "            Task(\n",
    "                description=\"Synthesize final response\",\n",
    "                agent=self.synthesizer,\n",
    "                expected_output='A JSON object containing ranked matches and summary',\n",
    "                context=[{\n",
    "                    \"description\": \"Create final response from all sources\",\n",
    "                    \"expected_output\": \"Final synthesized results\",\n",
    "                    \"query\": query,\n",
    "                    \"graph_results\": \"Will be filled from KG task\",\n",
    "                    \"doc_results\": \"Will be filled from retrieval task\"\n",
    "                }]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Execute the crew's tasks\n",
    "        try:\n",
    "            results = self.crew.execute(tasks)\n",
    "            return results[-1]  # Return the synthesizer's results\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing crew tasks: {e}\")\n",
    "            return {\n",
    "                \"matches\": [],\n",
    "                \"summary\": \"Error occurred during processing\"\n",
    "            }\n",
    "            \n",
    "def setup_enhanced_rag():\n",
    "    # Initialize components\n",
    "    documents = load_and_process_documents('./OKWs/')\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    kg = MakerspaceKG(embeddings)\n",
    "    \n",
    "    # Add some test data to the knowledge graph\n",
    "    kg.add_node_with_embedding(\n",
    "        name=\"Woodworking Shop\",\n",
    "        description=\"Specializes in sustainable wood processing and furniture making\",\n",
    "        capabilities=[\"woodworking\", \"furniture making\"],\n",
    "        resources=[\"sustainable wood\"]\n",
    "    )\n",
    "    \n",
    "    kg.add_node_with_embedding(\n",
    "        name=\"Textile Studio\",\n",
    "        description=\"Focuses on organic fabric processing and sustainable textile work\",\n",
    "        capabilities=[\"textile processing\"],\n",
    "        resources=[\"organic fabrics\"]\n",
    "    )\n",
    "    \n",
    "    vector_store = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "    llm = HuggingFacePipeline(pipeline=pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"google/flan-t5-base\"\n",
    "    ))\n",
    "    \n",
    "    #search_agent = SearchAgent(llm, kg, vector_store)\n",
    "    #matching_agent = MakerspaceMatchingAgent(search_agent)\n",
    "    matching_agent = EnhancedMakerspaceMatchingAgent(llm, kg, vector_store)\n",
    "    \n",
    "    return matching_agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    matching_agent = setup_enhanced_rag()\n",
    "    query = 'I need a makerspace with woodworking capabilities with sustainable and eco-friendly materials'\n",
    "    results = matching_agent.find_optimal_makerspaces(query)\n",
    "\n",
    "#matches = matching_agent.find_optimal_makerspaces(requirements)\n",
    "\n",
    "    print(\"Found matches:\")\n",
    "    for match in results.get(\"matches\", []):\n",
    "        print(f\"\\nTitle: {match['title']}\")\n",
    "        print(f\"Relevance Score: {match['relevance_score']}\")\n",
    "        print(f\"Explanation: {match['explanation']}\")\n",
    "        print(f\"Capabilities: {', '.join(match['capabilities'])}\")\n",
    "        print(f\"Resources: {', '.join(match['resources'])}\")\n",
    "    \n",
    "    print(f\"\\nSummary: {results.get('summary', 'No summary available')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f410f-0e7e-49e5-9bb4-4323fee81158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
