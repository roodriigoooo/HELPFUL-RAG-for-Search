{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be763ac-f2be-4f60-b412-80d2d48335ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from networkx import DiGraph\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "import glob\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from semantic_rag import SemanticJSONSplitter, load_and_process_documents\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b161a7-d4d3-46cb-9fe2-5919c9283786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryType(Enum):\n",
    "    SEMANTIC = \"semantic\"\n",
    "    KEYWORD = \"keyword\"\n",
    "    GRAPH = \"graph\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class MakerspaceKG:\n",
    "    def __init__(self, embeddings: HuggingFaceEmbeddings):\n",
    "        self.graph = DiGraph()\n",
    "        self.node_embeddings = {}\n",
    "        self.embeddings = embeddings  # i use the same original embeddings model\n",
    "    \n",
    "    def get_subgraph_for_query(self, query: str) -> DiGraph:\n",
    "        # get query embedding using our embedding model instance\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Find relevant nodes\n",
    "        relevant_nodes = []\n",
    "        for node in self.graph.nodes:\n",
    "            if node in self.node_embeddings:\n",
    "                similarity = self._calculate_similarity(\n",
    "                    query_embedding, \n",
    "                    self.node_embeddings[node]\n",
    "                )\n",
    "                if similarity > 0.5:  # Threshold can be adjusted\n",
    "                    relevant_nodes.append(node)\n",
    "        \n",
    "        return nx.subgraph(self.graph, relevant_nodes)\n",
    "\n",
    "    def add_node_with_embedding(self, name: str, description: str, **attrs):\n",
    "        \"\"\"Add a node and compute its embedding\"\"\"\n",
    "        self.graph.add_node(name, **attrs)\n",
    "        self.node_embeddings[name] = self.embeddings.embed_query(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1c85a4-dd37-4a58-9444-f6a059cbe475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRAGRouter:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    def determine_query_type(self, query: str) -> QueryType:\n",
    "        # Analyze query complexity and structure\n",
    "        query_features = self._extract_query_features(query)\n",
    "        \n",
    "        if query_features['requires_graph_traversal']:\n",
    "            return QueryType.GRAPH\n",
    "        elif query_features['is_semantic']:\n",
    "            return QueryType.SEMANTIC\n",
    "        elif query_features['is_keyword_based']:\n",
    "            return QueryType.KEYWORD\n",
    "        else:\n",
    "            return QueryType.HYBRID\n",
    "    \n",
    "    def _extract_query_features(self, query: str) -> Dict[str, bool]:\n",
    "        # Implement query analysis logic\n",
    "        # This is a simplified version\n",
    "        words = query.lower().split()\n",
    "        \n",
    "        return {\n",
    "            'requires_graph_traversal': any(w in words for w in ['related', 'connected', 'similar']),\n",
    "            'is_semantic': len(words) > 5 and ' '.join(words).find(' with ') != -1,\n",
    "            'is_keyword_based': len(words) <= 3\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7d765a-6dec-4bd2-8ed3-77c107776cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAgent:\n",
    "    def __init__(self, llm, knowledge_graph: MakerspaceKG, vector_store):\n",
    "        self.llm = llm\n",
    "        self.kg = knowledge_graph\n",
    "        self.vector_store = vector_store\n",
    "        self.router = AutoRAGRouter()\n",
    "        \n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        # Determine search strategy\n",
    "        query_type = self.router.determine_query_type(query)\n",
    "        \n",
    "        # Execute appropriate search strategy\n",
    "        if query_type == QueryType.GRAPH:\n",
    "            results = self._graph_search(query)\n",
    "        elif query_type == QueryType.SEMANTIC:\n",
    "            results = self._semantic_search(query)\n",
    "        elif query_type == QueryType.KEYWORD:\n",
    "            results = self._keyword_search(query)\n",
    "        else:\n",
    "            results = self._hybrid_search(query)\n",
    "\n",
    "        if results is None:\n",
    "            results = []\n",
    "\n",
    "        return self._format_results(results)\n",
    "    \n",
    "    def _graph_search(self, query: str) -> List[Any]:\n",
    "        try:\n",
    "            subgraph = self.kg.get_subgraph_for_query(query, self.model)\n",
    "            if not subgraph:\n",
    "                return []\n",
    "\n",
    "            results = []\n",
    "            for node in subgraph.nodes:\n",
    "                node_data = subgraph.nodes[node]\n",
    "                results.append({\n",
    "                    'name': node,\n",
    "                    'type': node_data.get('type', 'unknown'),\n",
    "                    'capabilities': node_data.get('capabilities', []),\n",
    "                    'resources': node_data.get('resources', [])\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Graph search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _semantic_search(self, query: str) -> List[Document]:\n",
    "        try:\n",
    "            results = self.vector_store.similarity_search(query)\n",
    "            return results if results else []\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _keyword_search(self, query: str) -> List[Document]:\n",
    "        # Implement basic keyword search using vector store\n",
    "        try:\n",
    "            # Split query into keywords\n",
    "            keywords = query.lower().split()\n",
    "            results = self.vector_store.similarity_search(\n",
    "                ' OR '.join(keywords),\n",
    "                k=5\n",
    "            )\n",
    "            return results if results else []\n",
    "        except Exception as e:\n",
    "            print(f\"Keyword search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _hybrid_search(self, query: str) -> List[Any]:\n",
    "        # Combine multiple search strategies\n",
    "        try:\n",
    "            graph_results = self._graph_search(query)\n",
    "            semantic_results = self._semantic_search(query)\n",
    "            return self._merge_results(graph_results, semantic_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Hybrid search error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _format_results(self, results: List[Any]) -> List[Dict[str, Any]]:\n",
    "        formatted_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            if isinstance(result, Document):\n",
    "                # Handle document objects (from langchain documents object type)\n",
    "                formatted_results.append({\n",
    "                    'title': result.metadata.get('title', 'No Title'),\n",
    "                    'content': result.page_content,\n",
    "                    'type': 'document',\n",
    "                    'metadata': result.metadata\n",
    "                })\n",
    "            elif isinstance(result, dict):\n",
    "                # Handle graph search results\n",
    "                formatted_results.append({\n",
    "                    'title': result.get('name', 'No Name'),\n",
    "                    'capabilities': result.get('capabilities', []),\n",
    "                    'resources': result.get('resources', []),\n",
    "                    'type': 'graph_node',\n",
    "                    'metadata': result\n",
    "                })\n",
    "                \n",
    "        return formatted_results\n",
    "    \n",
    "    def _merge_results(self, graph_results: List[Any], semantic_results: List[Document]) -> List[Any]:\n",
    "        \"\"\"Merge results from different search strategies\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Add graph results\n",
    "        all_results.extend(graph_results)\n",
    "        \n",
    "        # Add semantic results, avoiding duplicates\n",
    "        seen_titles = {r.get('title') for r in all_results}\n",
    "        for doc in semantic_results:\n",
    "            if doc.metadata.get('title') not in seen_titles:\n",
    "                all_results.append(doc)\n",
    "                seen_titles.add(doc.metadata.get('title'))\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "class MakerspaceMatchingAgent:\n",
    "    def __init__(self, search_agent: SearchAgent):\n",
    "        self.search_agent = search_agent\n",
    "        \n",
    "    def find_optimal_makerspaces(self, requirements: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        # Analyze requirements\n",
    "        required_capabilities = requirements.get('capabilities', [])\n",
    "        required_resources = requirements.get('resources', [])\n",
    "        \n",
    "        # Search for potential matches\n",
    "        matches = []\n",
    "        for capability in required_capabilities:\n",
    "            results = self.search_agent.search(capability)\n",
    "            matches.extend(results)\n",
    "        \n",
    "        # here, we would ideally score and rank matches, we can use an open-source reranking model, or develop the algorithm by scratch\n",
    "        #ranked_matches = self._rank_matches(matches, requirements)\n",
    "        #return ranked_matches\n",
    "        return matches\n",
    "    \n",
    "    def _rank_matches(self, matches: List[Dict[str, Any]], \n",
    "                     requirements: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        # Implement ranking logic\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc78092-e4c6-48ec-bf6a-2dde5cdf4811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rsast\\AppData\\Local\\Temp\\ipykernel_11396\\4243756485.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "C:\\Users\\rsast\\AppData\\Local\\Temp\\ipykernel_11396\\4243756485.py:27: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline(\n"
     ]
    }
   ],
   "source": [
    "from semantic_rag import SemanticJSONSplitter, load_and_process_documents\n",
    "\n",
    "def setup_enhanced_rag():\n",
    "    # Initialize components\n",
    "    documents = load_and_process_documents('./OKWs/')\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    kg = MakerspaceKG(embeddings)\n",
    "    \n",
    "    # Add some test data to the knowledge graph\n",
    "    kg.add_node_with_embedding(\n",
    "        name=\"Woodworking Shop\",\n",
    "        description=\"Specializes in sustainable wood processing and furniture making\",\n",
    "        capabilities=[\"woodworking\", \"furniture making\"],\n",
    "        resources=[\"sustainable wood\"]\n",
    "    )\n",
    "    \n",
    "    kg.add_node_with_embedding(\n",
    "        name=\"Textile Studio\",\n",
    "        description=\"Focuses on organic fabric processing and sustainable textile work\",\n",
    "        capabilities=[\"textile processing\"],\n",
    "        resources=[\"organic fabrics\"]\n",
    "    )\n",
    "    \n",
    "    vector_store = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "    llm = HuggingFacePipeline(pipeline=pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"google/flan-t5-base\"\n",
    "    ))\n",
    "    \n",
    "    search_agent = SearchAgent(llm, kg, vector_store)\n",
    "    matching_agent = MakerspaceMatchingAgent(search_agent)\n",
    "    \n",
    "    return matching_agent\n",
    "\n",
    "# Example usage\n",
    "matching_agent = setup_enhanced_rag()\n",
    "requirements = {\n",
    "    \"capabilities\": [\"woodworking\", \"textile processing\"],\n",
    "    \"resources\": [\"sustainable wood\", \"organic fabrics\"],\n",
    "    \"preferences\": {\n",
    "        \"location\": \"local\",\n",
    "        \"sustainability\": \"high\"\n",
    "    }\n",
    "}\n",
    "\n",
    "matches = matching_agent.find_optimal_makerspaces(requirements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
